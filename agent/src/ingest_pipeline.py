import json
import chromadb
import torch
import pickle
import warnings
import os
from model import TripMindEncoder
from utils import get_semantic_vector

# T·∫Øt c·∫£nh b√°o phi√™n b·∫£n sklearn kh√¥ng kh·ªõp
warnings.filterwarnings("ignore")

# C·∫•u h√¨nh thi·∫øt b·ªã
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ ƒêang ch·∫°y Ingest tr√™n thi·∫øt b·ªã: {DEVICE}")

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
ASSETS_PATH = "/Users/trannguyenmyanh/Documents/TripMind/agent/weights/assets.pkl"
WEIGHTS_PATH = "/Users/trannguyenmyanh/Documents/TripMind/agent/weights/encoder_weights.pth"
DB_PATH = "/Users/trannguyenmyanh/Documents/TripMind/agent/tripmind_vector_db"
DATA_PATH = "/Users/trannguyenmyanh/Documents/TripMind/data/cleaned_data.jsonl"

def load_encoder():
    """Kh·ªüi t·∫°o model v·ªõi c·∫•u h√¨nh chu·∫©n d_model=256"""
    with open(ASSETS_PATH, "rb") as f:
        assets = pickle.load(f)
    
    vocab_size = assets['vocab_size']
    num_categories = len(assets['cat_encoder'].classes_)
    
    # Ph·∫£i kh·ªõp d_model=256 v·ªõi Agent 1
    encoder = TripMindEncoder(
        vocab_size=vocab_size,
        num_categories=num_categories,
        d_model=256, 
        nhead=8,
        num_layers=4
    )
    
    encoder.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE))
    encoder.to(DEVICE).eval()
    return encoder, assets

def ingest_data():
    encoder, assets = load_encoder()
    client = chromadb.PersistentClient(path=DB_PATH)

    # L√†m m·ªõi collection
    try:
        client.delete_collection("tripmind_reviews")
        print("--- ƒê√£ x√≥a collection c≈© ---")
    except:
        pass

    collection = client.create_collection(
        name="tripmind_reviews", 
        metadata={"hnsw:space": "cosine"}
    )

    batch_size = 100
    ids, docs, metas, embs = [], [], [], []

    print(f"üèÅ B·∫Øt ƒë·∫ßu n·∫°p d·ªØ li·ªáu t·ª´: {DATA_PATH}")

    with open(DATA_PATH, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            try:
                data = json.loads(line)
                
                # 1. L·∫•y th√¥ng tin c∆° b·∫£n
                name = data.get('name', 'Unknown').strip()
                review_text = data.get('text', '').strip()
                p_id = str(data.get("province_id", "")).zfill(2) # Chu·∫©n h√≥a "01", "10"
                
                if not review_text and not name:
                    continue

                # 2. X·ª≠ l√Ω Categories & Trip Type cho Metadata
                # L·∫•y category ƒë·∫ßu ti√™n n·∫øu c√≥
                cats = data.get('categories', [])
                cat_name = cats[0].get('name', 'Kh√°c') if isinstance(cats, list) and cats else "Kh√°c"
                
                # L√†m s·∫°ch Trip Type
                trip_raw = data.get("trip", "{}")
                trip_data = json.loads(trip_raw) if isinstance(trip_raw, str) else (trip_raw or {})
                trip_type = str(trip_data.get("trip_type", "any")).lower()

                # 3. QUAN TR·ªåNG: T·∫°o Rich Text ƒë·ªÉ tƒÉng c∆∞·ªùng ng·ªØ nghƒ©a
                # G·ªôp Name + Category + Review ƒë·ªÉ Model t√¨m ki·∫øm hi·ªáu qu·∫£ theo t√™n ƒë·ªãa danh
                rich_text = f"ƒê·ªãa danh: {name}. Lo·∫°i h√¨nh: {cat_name}. ƒê√°nh gi√°: {review_text}".lower()

                # 4. T·∫°o Vector Embedding t·ª´ Rich Text
                vector = get_semantic_vector(rich_text, encoder, assets, DEVICE)

                # 5. Chu·∫©n b·ªã Metadata ƒë·ªÉ l·ªçc (province_id l√† ti√™u ch√≠ ch√≠nh)
                metadata = {
                    "province_id": p_id,
                    "destination_id": str(data.get("destination_id")),
                    "name": name,
                    "category": cat_name,
                    "trip_type": trip_type,
                    "rating": float(data.get("rating_x", 0))
                }

                ids.append(str(data.get('id_review', f"rev_{i}")))
                docs.append(review_text) # L∆∞u review g·ªëc ƒë·ªÉ hi·ªÉn th·ªã
                metas.append(metadata)
                embs.append(vector)

                if len(ids) >= batch_size:
                    collection.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)
                    if (i + 1) % 500 == 0:
                        print(f"‚úÖ ƒê√£ n·∫°p {i+1} b·∫£n ghi...")
                    ids, docs, metas, embs = [], [], [], []

            except Exception as e:
                continue

    # N·∫°p ph·∫ßn d∆∞ c√≤n l·∫°i
    if ids:
        collection.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)
    
    print(f"üéâ Ho√†n th√†nh! T·ªïng c·ªông: {collection.count()} b·∫£n ghi ƒë√£ s·∫µn s√†ng cho Agent 1.")

if __name__ == "__main__":
    ingest_data()