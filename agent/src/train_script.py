import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pickle
import os
import re
from tqdm import tqdm
import matplotlib.pyplot as plt
from model import TripMindEncoder
from dataset import TripMindDataset

# Khá»Ÿi táº¡o danh sÃ¡ch lÆ°u lá»‹ch sá»­
history = {
    'train_loss': [],
    'train_acc': []
}

# --- Cáº¤U HÃŒNH ---
DATA_PATH = "/kaggle/input/dl-dataset/cleaned_data.jsonl"
WEIGHTS_DIR = "/kaggle/working/"
MAX_SEQ_LEN = 100
BATCH_SIZE = 32
EPOCHS = 100  # TÄƒng lÃªn Ä‘á»ƒ tháº¥y sá»± há»™i tá»¥
LEARNING_RATE = 5e-5
D_MODEL = 256
NHEAD = 8
NUM_LAYERS = 4 
best_acc = 0.0  

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- TIá»€N Xá»¬ LÃ VÄ‚N Báº¢N ---
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)
    return text.split()


def train():
    global best_acc
    print(f"ðŸš€ Khá»Ÿi táº¡o Multi-task Learning trÃªn {device}...")
    
    dataset = TripMindDataset(DATA_PATH)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    vocab_size = len(dataset.word2idx)
    num_destinations = len(dataset.label_encoder.classes_)
    num_categories = len(dataset.cat_encoder.classes_)
    
    print(f"ðŸ“Š Vocab: {vocab_size} | Äá»‹a danh: {num_destinations} | Loáº¡i hÃ¬nh: {num_categories}")

    # Khá»Ÿi táº¡o Model vá»›i 2 Ä‘áº§u ra
    model = TripMindEncoder(
        vocab_size=vocab_size, 
        num_categories=num_categories, # Truyá»n vÃ o Ä‘á»ƒ kÃ­ch hoáº¡t classifier
        d_model=D_MODEL, 
        nhead=NHEAD, 
        num_layers=NUM_LAYERS
    ).to(device)
    
    # Classifier phá»¥ cho Destination ID
    dest_classifier = nn.Linear(D_MODEL, num_destinations).to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(list(model.parameters()) + list(dest_classifier.parameters()), lr=LEARNING_RATE)

    for epoch in range(EPOCHS):
        model.train()
        total_loss, correct, total = 0, 0, 0
        loop = tqdm(dataloader, desc=f"Epoch [{epoch+1}/{EPOCHS}]")
        
        for texts, labels_dist, labels_cat in loop:
            texts, labels_dist, labels_cat = texts.to(device), labels_dist.to(device), labels_cat.to(device)
            
            optimizer.zero_grad()
            
            # Forward pass: Model tráº£ vá» Embedding vÃ  Logits cá»§a Category
            emb, cat_logits = model(texts)
            dest_outputs = dest_classifier(emb)
            
            # TÃ­nh toÃ¡n 2 loáº¡i Loss
            loss_dist = criterion(dest_outputs, labels_dist) # PhÃ¢n biá»‡t Ä‘á»‹a danh
            loss_cat = criterion(cat_logits, labels_cat)    # PhÃ¢n biá»‡t loáº¡i hÃ¬nh (ChÃ¹a vs Biá»ƒn)
            
            # Loss tá»•ng há»£p (Æ¯u tiÃªn há»c ngá»¯ nghÄ©a Category vá»›i trá»ng sá»‘ 2.0)
            batch_loss = loss_dist + (2.0 * loss_cat)
            
            batch_loss.backward()
            optimizer.step()
            
            total_loss += batch_loss.item()
            
            # TÃ­nh accuracy dá»±a trÃªn Destination (Ä‘á»ƒ so sÃ¡nh vá»›i báº£n cÅ©)
            _, predicted = torch.max(dest_outputs.data, 1)
            total += labels_dist.size(0)
            correct += (predicted == labels_dist).sum().item()
            
            epoch_acc = 100 * correct / total
            loop.set_postfix(loss=f"{total_loss/len(dataloader):.4f}", acc=f"{epoch_acc:.2f}%")

        # LÆ°u lá»‹ch sá»­ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“
        history['train_loss'].append(total_loss/len(dataloader))
        history['train_acc'].append(epoch_acc)

        if epoch_acc > best_acc:
            best_acc = epoch_acc
            torch.save(model.state_dict(), os.path.join(WEIGHTS_DIR, "encoder_weights.pth"))
            print(f"ðŸŒŸ Best Model Updated: {best_acc:.2f}%")

    # LÆ°u Assets bao gá»“m cáº£ cat_encoder
    assets = {
        "word2idx": dataset.word2idx,
        "vocab_size": vocab_size,
        "label_encoder": dataset.label_encoder,
        "cat_encoder": dataset.cat_encoder,
        "d_model": D_MODEL,
        "nhead": NHEAD,
        "num_layers": NUM_LAYERS
    }
    with open(os.path.join(WEIGHTS_DIR, "assets.pkl"), "wb") as f:
        pickle.dump(assets, f)

if __name__ == "__main__":
    train()